{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32079102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-10-20T14:44:39.748885Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1248452584, 'load_duration': 940656750, 'prompt_eval_count': 32, 'prompt_eval_duration': 223917875, 'eval_count': 8, 'eval_duration': 78302333, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--7c2cc67c-6fd2-4fcc-836e-7c7d6401483d-0', usage_metadata={'input_tokens': 32, 'output_tokens': 8, 'total_tokens': 40})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "llm.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87b02e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab43bb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='What is the capital of France?'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"What is the capital of {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"country\": \"France\"})\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e85892c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='What is the capital of France?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-10-20T14:20:03.356509Z', 'done': True, 'done_reason': 'stop', 'total_duration': 260924334, 'load_duration': 106188292, 'prompt_eval_count': 32, 'prompt_eval_duration': 71193958, 'eval_count': 8, 'eval_duration': 78959542, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--98ad53aa-6221-4a0c-b00b-0a3baec3aace-0', usage_metadata={'input_tokens': 32, 'output_tokens': 8, 'total_tokens': 40})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"What is the capital of {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"country\": \"France\"})\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fb5cc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Paris, the City of Light! It\\'s a world-renowned city known for its stunning architecture, rich history, art museums, fashion, and romantic atmosphere. Here are some interesting facts and things to see in Paris:\\n\\n**Must-visit attractions:**\\n\\n1. The Eiffel Tower: An iconic iron lattice tower that offers breathtaking views of the city.\\n2. The Louvre Museum: Home to the Mona Lisa and many other famous artworks.\\n3. Notre-Dame Cathedral: A beautiful Gothic church that was built in the 12th century.\\n4. Arc de Triomphe: A monumental arch that honors the soldiers who fought and died for France.\\n5. Champs-Élysées: A famous avenue lined with high-end boutiques, cafes, and theaters.\\n\\n**Neighborhoods to explore:**\\n\\n1. Montmartre: A historic, artistic neighborhood known for its narrow streets, charming squares, and stunning views of the city.\\n2. Le Marais: A trendy neighborhood with a mix of upscale boutiques, art galleries, and hip restaurants.\\n3. Belleville: A vibrant neighborhood with a mix of bohemian style, street food, and international cuisine.\\n\\n**Things to do:**\\n\\n1. Take a river cruise along the Seine River to see the city\\'s landmarks from a different perspective.\\n2. Visit the Musée d\\'Orsay, which houses an impressive collection of Impressionist and Post-Impressionist art.\\n3. Explore the Latin Quarter, where you can find charming cafes, bookstores, and street performers.\\n4. Go shopping on the Champs-Élysées or in the upscale boutiques of the Galeries Lafayette department store.\\n\\n**Food and drink:**\\n\\n1. Indulge in French cuisine, which is famous for its rich flavors, intricate preparations, and fine wines.\\n2. Try a classic croissant or baguette at a local bakery.\\n3. Visit a wine bar or champagne cellar to taste some of France\\'s finest vintages.\\n\\n**Tips and tricks:**\\n\\n1. Get familiar with the city\\'s public transportation system, including the métro and buses.\\n2. Learn some basic French phrases, such as \"bonjour\" (hello), \"merci\" (thank you), and \"excusez-moi\" (excuse me).\\n3. Be prepared for crowds and lines at popular attractions.\\n\\nParis is a city that has something for everyone, from art lovers to foodies to history buffs and romantics. It\\'s a place where tradition meets innovation, and where the air is always alive with energy and creativity.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-10-20T14:33:18.365532Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6678687416, 'load_duration': 106123458, 'prompt_eval_count': 63, 'prompt_eval_duration': 78210416, 'eval_count': 529, 'eval_duration': 6233594180, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--b511e96c-d011-4ace-8294-f43502f9e5ff-0', usage_metadata={'input_tokens': 63, 'output_tokens': 529, 'total_tokens': 592})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "message_list = [\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),\n",
    "    HumanMessage(content=\"Can you tell me more about Paris?\"),\n",
    "]\n",
    "\n",
    "llm.invoke(message_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d377ee62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={})]\n",
      "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-10-20T14:36:32.437975Z', 'done': True, 'done_reason': 'stop', 'total_duration': 253843875, 'load_duration': 105874875, 'prompt_eval_count': 38, 'prompt_eval_duration': 63263250, 'eval_count': 8, 'eval_duration': 79638750, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--c3c95908-7243-417a-ab0c-320b0e375bd9-0', usage_metadata={'input_tokens': 38, 'output_tokens': 8, 'total_tokens': 46})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\" ),\n",
    "    (\"human\", \"What is the capital of {country}?\"),\n",
    "])\n",
    "\n",
    "chat_prompt = chat_prompt_template.invoke({\"country\": \"France\"})\n",
    "\n",
    "print(chat_prompt)\n",
    "print(chat_prompt.messages)\n",
    "llm.invoke(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe2527c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='What is the capital of France?'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"What is the capital of {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"country\": \"France\"})\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "ai_message = llm.invoke(prompt_template.invoke({\"country\": \"France\"}))\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "answer = output_parser.invoke(llm.invoke(prompt_template.invoke({\"country\": \"France\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e6094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5baa0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-10-20T14:45:36.364931Z', 'done': True, 'done_reason': 'stop', 'total_duration': 291799917, 'load_duration': 104483875, 'prompt_eval_count': 32, 'prompt_eval_duration': 100126583, 'eval_count': 8, 'eval_duration': 82396749, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--d8bd456a-1756-4f7e-b82b-4fbf8f9712ee-0', usage_metadata={'input_tokens': 32, 'output_tokens': 8, 'total_tokens': 40})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_message # BaseMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eb6d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer # String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be6b6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Invalid json output: ```\n{\n    \"capital\": \"Paris\",\n    \"population\": 65_242_000,\n    \"official_languages\": [\"French\", \"Occitan\"],\n    \"currency\": \"Euro\"\n}\n```\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.14/lib/python3.11/site-packages/langchain_core/output_parsers/json.py:81\u001b[39m, in \u001b[36mJsonOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.14/lib/python3.11/site-packages/langchain_core/utils/json.py:154\u001b[39m, in \u001b[36mparse_json_markdown\u001b[39m\u001b[34m(json_string, parser)\u001b[39m\n\u001b[32m    153\u001b[39m     json_str = json_string \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m match.group(\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.14/lib/python3.11/site-packages/langchain_core/utils/json.py:170\u001b[39m, in \u001b[36m_parse_json\u001b[39m\u001b[34m(json_str, parser)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.14/lib/python3.11/site-packages/langchain_core/utils/json.py:127\u001b[39m, in \u001b[36mparse_partial_json\u001b[39m\u001b[34m(s, strict)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.14/lib/python3.11/json/__init__.py:359\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    358\u001b[39m     kw[\u001b[33m'\u001b[39m\u001b[33mparse_constant\u001b[39m\u001b[33m'\u001b[39m] = parse_constant\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.14/lib/python3.11/json/decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.14/lib/python3.11/json/decoder.py:353\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting ',' delimiter: line 3 column 21 (char 46)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOutputParserException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     17\u001b[39m output_parser = JsonOutputParser()\n\u001b[32m     19\u001b[39m json_ai_message = llm.invoke(country_detail_prompt.invoke({\u001b[33m\"\u001b[39m\u001b[33mcountry\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mFrance\u001b[39m\u001b[33m\"\u001b[39m}))\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43moutput_parser\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_ai_message\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.14/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:197\u001b[39m, in \u001b[36mBaseOutputParser.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    191\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m     **kwargs: Any,\n\u001b[32m    195\u001b[39m ) -> T:\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    206\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    207\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    208\u001b[39m         config,\n\u001b[32m    209\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    210\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.14/lib/python3.11/site-packages/langchain_core/runnables/base.py:2021\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2017\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2018\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2019\u001b[39m         output = cast(\n\u001b[32m   2020\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2021\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2022\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2023\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2024\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2025\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2026\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2028\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2029\u001b[39m         )\n\u001b[32m   2030\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2031\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.14/lib/python3.11/site-packages/langchain_core/runnables/config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.14/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:198\u001b[39m, in \u001b[36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[39m\u001b[34m(inner_input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    191\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m     **kwargs: Any,\n\u001b[32m    195\u001b[39m ) -> T:\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m    197\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    201\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    202\u001b[39m             config,\n\u001b[32m    203\u001b[39m             run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    204\u001b[39m         )\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    206\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    207\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    208\u001b[39m         config,\n\u001b[32m    209\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    210\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.14/lib/python3.11/site-packages/langchain_core/output_parsers/json.py:84\u001b[39m, in \u001b[36mJsonOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     83\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid json output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg, llm_output=text) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOutputParserException\u001b[39m: Invalid json output: ```\n{\n    \"capital\": \"Paris\",\n    \"population\": 65_242_000,\n    \"official_languages\": [\"French\", \"Occitan\"],\n    \"currency\": \"Euro\"\n}\n```\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# JSON Output Parser는 쓰지 않는 것이 좋다.\n",
    "country_detail_prompt = PromptTemplate(\n",
    "    template=\"\"\"Give following information about {country}:\n",
    "        - Capital\n",
    "        - Population\n",
    "        - Official Languages\n",
    "        - Currency\n",
    "\n",
    "        return it in JSON format. and return the JSON dictionary only\n",
    "    \"\"\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "country_detail_prompt.invoke({\"country\": \"France\"})\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "json_ai_message = llm.invoke(country_detail_prompt.invoke({\"country\": \"France\"})) \n",
    "\n",
    "output_parser.invoke(json_ai_message) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4c950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```\\n{\\n    \"capital\": \"Paris\",\\n    \"population\": 65_242_000,\\n    \"official_languages\": [\"French\", \"Occitan\"],\\n    \"currency\": \"Euro\"\\n}\\n```' additional_kwargs={} response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-10-20T14:51:09.952952Z', 'done': True, 'done_reason': 'stop', 'total_duration': 755463959, 'load_duration': 108302167, 'prompt_eval_count': 63, 'prompt_eval_duration': 156656125, 'eval_count': 43, 'eval_duration': 464401041, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'} id='lc_run--9cbf6016-dac8-4fc0-9aca-e1be04875043-0' usage_metadata={'input_tokens': 63, 'output_tokens': 43, 'total_tokens': 106}\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(json_ai_message)\n",
    "print(type(json_ai_message.content)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b401bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd577e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee70e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac7512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b7d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e49ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79f776e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
